{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openspyrit/spyrit-examples/blob/master/tutorial/tuto_train_lin_meas_colab.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial to train a reconstruction network "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial to train a reconstruction network for 2D single-pixel imaging on stl10, for linear measurements. In specific, we choose a Hadamard positive matrix, but this can be replaced by any matrix. \n",
    "\n",
    "Training is performed by a call to *train.py*. Several parameters allow to modify acquisition, network and training (network architecture), optimisation and the use of tensorboard. \n",
    "\n",
    "Currently you can train the following networks by modifying the network architecture variable *arch*: \n",
    "\n",
    "- 'dc-net': Denoised Completion Network (DCNet). \n",
    "- 'pinv-net': Pseudo Inverse Network (PinvNet).\n",
    "- 'upgd': Unrolled proximal gradient descent (UPGD). \n",
    "\n",
    "and the denoising variable *denoi*: \n",
    "- 'cnn': CNN no batch normalization\n",
    "- 'cnnbn': CNN with batch normalization\n",
    "- 'unet': UNet (0.5 M trainable parameters)\n",
    "- 'drunet': DRUNet (high capacity residual UNet that allows training for all noise levels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set google colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On colab, choose GPU at *Runtime/Change runtime type*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, mount google drive to import modules spyrit modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_colab = True\n",
    "if (mode_colab is True):\n",
    "    # Connect to googledrive\n",
    "    #if 'google.colab' in str(get_ipython()):\n",
    "    # Mount google drive to access files via colab\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    %cd /content/gdrive/MyDrive/\n",
    "\n",
    "    # For the profiler\n",
    "    !pip install -U tensorboard-plugin-profile\n",
    "\n",
    "    # Load the TensorBoard notebook extension\n",
    "    %load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone Spyrit package"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone and install spyrit package if not installed or change to spyrit folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (mode_colab is True):\n",
    "    # Clone and install\n",
    "    !git clone https://github.com/openspyrit/spyrit.git\n",
    "    %cd spyrit\n",
    "    !pip install -e .\n",
    "\n",
    "    # Checkout to ongoing branch\n",
    "    !git fetch --all\n",
    "\n",
    "    # Add paths for modules\n",
    "    import sys\n",
    "    sys.path.append('./spyrit/core')\n",
    "    sys.path.append('./spyrit/misc')\n",
    "    sys.path.append('./spyrit/tutorial')\n",
    "    %cd ..\n",
    "\n",
    "    # Clone Spyrit-examples and checkout to branch tutorials\n",
    "    !git clone https://github.com/openspyrit/spyrit-examples.git\n",
    "    %cd spyrit-examples/tutorial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data and training parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose the following parameters:\n",
    "- Measurements type (forward):\n",
    "    - --meas: Measurement operator: 'hadam-split', 'hadam-pos'. Default=\"hadam-split\" \n",
    "    - --noise: Noise operator: 'poisson', 'gauss-approx', 'no-noise'. Default=\"poisson\"\n",
    "    - --prep: Preprocessing operator: 'dir-poisson', 'split-poisson'. Default=\"dir-poisson\"\n",
    "\n",
    "- Acquisition: \n",
    "    - --img_size: Height / width dimension, default=64\n",
    "    - --M: Number of undersampling patterns, default=512\n",
    "    - --subs: Among 'var','rect', default=\"var\"\n",
    "\n",
    "- Network and training: \n",
    "    - --data: stl10 or imagenet, default=\"stl10\"\n",
    "    - --model_root: Path to model saving files, default='./model/'\n",
    "    - --data_root: Path to the dataset, default=\"./data/\"\n",
    "\n",
    "    - --N0: Mean maximum total number of photons, default=10\n",
    "    - --stat_root: Path to precomputed data (cov matrix), default=\"\"\n",
    "    - --arch: Choose among 'dc-net','pinv-net', 'upgd', default=\"dc-net\"\n",
    "    - --denoi: Choose among 'cnn','cnnbn', 'unet', default=\"unet\"\n",
    "\n",
    "- Optimisation:\n",
    "    - --num_epochs: Number of training epochs, default=30\n",
    "    - --batch_size: Size of each training batch, default=512\n",
    "    - --reg: Regularisation Parameter, default=1e-7\n",
    "    - --step_size: Scheduler Step Size, default=10\n",
    "    - --gamma: Scheduler Decrease Rate, default=0.5\n",
    "    - --checkpoint_model: Optional path to checkpoint model, default=\"\"\n",
    "    - --checkpoint_interval: Interval between saving model checkpoints, default=0\n",
    "    - Training is done with *Adam* optimizer, *MSELoss*\n",
    "\n",
    "- Tensorboard:\n",
    "    - --tb_path: Relative path for Tensorboard experiment tracking logs, default=False\n",
    "    - --tb_prof: Code profiler with Tensorboard, default=False\n",
    "    - Logging of scalars *train_loss*, *val_loss* and images (dataset example ground-truth and predictions at different epochs).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we consider noiseless data (1 mean photons) and an undersampling factor of 4. Training is done on stl10 dataset with default parameters and using experiment tracking with tensorboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# (the first three paramaters allow to generalize train_gen_meas.py \n",
    "# to common measurement types)\n",
    "meas = 'hadam-pos'    # measurement type\n",
    "noise = 'no-noise' # noise type\n",
    "prep = 'dir-poisson'    # preprocessing type\n",
    "#\n",
    "N0 = 1.0        # ph/pixel max: number of counts\n",
    "img_size = 64   # image size\n",
    "M =  img_size**2 // 4  # Num measurements = subsampled by factor 4\n",
    "data_root = './data/'\n",
    "data = 'stl10'\n",
    "arch = 'pinv-net' # Network architecture\n",
    "denoi = 'cnn' # Denoiser architecture\n",
    "num_epochs = 30\n",
    "\n",
    "# Tensorboard logs path\n",
    "name_run = \"stdl10_hadampos\"\n",
    "mode_tb = True\n",
    "if (mode_tb is True):\n",
    "    now = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "    tb_path = f'runs/runs_{name_run}_n{int(N0)}_m{M}/{now}'\n",
    "    print(f\"Tensorboard logdir {tb_path}\")\n",
    "else:\n",
    "    tb_path = None\n",
    "    \n",
    "tb_prof = False # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, training is done by calling `train_gen_meas.py`, which handles all the data, model definitions and training parameters for the provided tutorials, and then it calls `train_model` from `spyrit.core.train` module. For personalized training, you may want to use only `train_model` and create your personalized script version of `train_gen_meas.py`. \n",
    "\n",
    "If you find problems executing `!python3 train_gen_meas.py` directly, you may also try `subprocess`, but results are not shown during training. You may visualize them using tensorboard, see below. \n",
    "\n",
    "Training time: \n",
    "- 2 min to download stl10\n",
    "- 2 min per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run train.py\n",
    "!python3 train_gen_meas.py --meas $meas --noise $noise --prep $prep --N0 $N0 --M $M --tb_path $tb_path --arch $arch --denoi $denoi --num_epochs $num_epochs\n",
    "\n",
    "#import subprocess\n",
    "#subprocess.run(['python3', 'train_gen_meas.py', '--meas', meas, '--noise', noise, '--prep', prep,\n",
    "#                '--N0', str(N0), '--M', str(M), \n",
    "#                '--arch', arch, '--denoi', denoi, '--num_epochs', str(num_epochs),\n",
    "#                '--tb_path', tb_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that logs are being save under `spyrit-examples/tutorial/runs` (clicking `Files` icon on your left pannel or directyly in your drive).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model is saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List model\n",
    "!ls -R model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List runs\n",
    "!ls -R runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can launch tensorboard to visualize tracked metrics and images. Select *SCALARS* or *IMAGES* to visualize losses/metrics and reconstructed images, respectively. More options are available in the top-right corner (CNN weights, profiling). \n",
    "\n",
    "You can launch tensorboard in another notebook *launch_tensorboard_colab.ipynb* during training but it may not always work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard\n",
    "# %tensorboard --logdir $tb_path\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If run twice tensorboard\n",
    "#!lsof -i:6006\n",
    "#!kill -9 17387"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close colab session!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to close colab session by deleting the instance at the upper menu *Runtime/Manage sessions/*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
