{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **Deep image reconstruction using the PyTorch and SPyRiT packages** NB: to be updated from main_with_answers.ipynb\n",
    "\n",
    "## <span style=\"color:brown\"> **With application to limited-angle computed tomography**\n",
    "\n",
    "*PyTorch 1.10.1; SPyRiT 1.1.0*\n",
    "\n",
    "Authors: N Ducros, T Leuliet, A Lorente Mur, Louise Friot--Giroux\n",
    "    \n",
    "Contact: *nicolas.ducros@creatis.insa-lyon.fr*\n",
    "    \n",
    "##  <span style=\"color:brown\"> Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spyrit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import scipy.io as sio\n",
    "import scipy.linalg as lin\n",
    "from PIL import Image, ImageOps\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import model_Radon_DCAN as model_radon\n",
    "import h5py as h5\n",
    "from skimage.data import shepp_logan_phantom\n",
    "from skimage.transform import radon, rescale\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Acquisition\n",
    "img_size = 64 # image size\n",
    "pixel_size = 64 #Number of pixels of the sensor\n",
    "\n",
    "#- Using CPU or GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **1 - Computed Tomography (CT) and Radon operator**\n",
    "\n",
    "## Computed Tomography (CT)\n",
    "Computed tomography (CT) is an imaging modality that reconstructs 2D or 3D objects from attenuation measurements. CT is a technique used in non-destructive inspection but most notably in medical imaging, where attenuation allows the type of tissue (e.g., bone, soft tissue) and structures (e.g., tumors) to be identified. The image formation process can be modelled by the Radon transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Radon transform\n",
    "The [Radon transform](https://en.wikipedia.org/wiki/Radon_transform) is an integral transform that returns line integrals over hyperplanes (e.g., along lines for a 2D object). We illustrate this process in a discrete setting below. We consider the projection of a discrete object image $\\mathbf{x}$ along a projection ray $(r_j, \\theta_k)$, where $\\{r_j\\}$ is the detector pixel locations and $\\theta_k$ the projection angle. The integral  measured for all detector pixels under all projection views is known as the '[sinogram](https://en.wikipedia.org/wiki/Radon_transform#/media/File:Radon_transform_sinogram.gif)'. \n",
    "\n",
    "<img src=\"fig/tomo.png\" alt=\"Projections schem\" style=\"width: 80%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: How does the sinogram of a point object (i.e., an image with only one nonzero pixel) look like? Complete the code below**\n",
    "    \n",
    "<font color='green'>**Help: Create a (img_size x img_size) image with only one pixel set to 1. Set all the other pixels to 0.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector with acquired angles\n",
    "theta = np.linspace(0., 180., 181)\n",
    "\n",
    "# Image with a single pixel set to one (point response function)\n",
    "example =  #### COMPLETE HERE #####\n",
    "\n",
    "\n",
    "# Sinogramm (CT measurements from object image)\n",
    "sinogram = radon(example, theta, circle=False)\n",
    "sinogram = rescale(sinogram, scale=(pixel_size/sinogram.shape[0],1), mode='reflect', multichannel=False)\n",
    "\n",
    "# Plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4.5))\n",
    "ax1.set_title(\"Object image\")\n",
    "ax1.imshow(example, cmap=plt.cm.Greys_r)\n",
    "\n",
    "ax2.set_title(\"Sinogram\")\n",
    "ax2.set_xlabel(\"Projection angle (deg)\")\n",
    "ax2.set_ylabel(\"Dectector pixel\")\n",
    "ax2.imshow(sinogram, cmap=plt.cm.Greys_r,\n",
    "           extent=(0, 180, 0, sinogram.shape[0]), aspect='auto')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Explain the 'sinogram' terminology.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> The Radon operator\n",
    "The Radon transform is a linear operator. Therefore, the sinogram $\\textbf{m}$ can be simply obtained as the matrix vector product $\\textbf{m} = \\textbf{Af}$, where $\\textbf{A}$ represent the discrete Radon (forward) operator and $\\textbf{f}$ is the object image. Both $\\textbf{m}$ and $\\textbf{f}$ are column vectors; $\\textbf{m}$ contains all the measurements, $\\textbf{f}$ all the image pixels. $\\textbf{A}$ is a matrix, whose dimensions match with the dimentions of $\\textbf{m}$ and $\\textbf{f}$.\n",
    "\n",
    "This is illustrated below. \n",
    "    \n",
    "<img src=\"fig/def var.jpg\" alt=\"m and A\" style=\"width: 50%;\"/> <img src=\"fig/dim.jpg\" alt=\"forward operator\" style=\"width: 49%;\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> Creating the operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective here is to create this forward operator $A$ for a toy example of images of size $16\\times 16$. \n",
    "\n",
    "<font color='blue'>**Q: Based on the explanation above, complete the code below to create the matrix 'A_example'**</font>.\n",
    "\n",
    "<font color='green'>**Help: First, determine the size of the Radon matrix**</font>\n",
    "\n",
    "<font color='green'>**Help: Next, generate one column of the Radon matrix at a time using the 'radon' function**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nb of angles of acquisition\n",
    "total_angles = 181\n",
    "\n",
    "# Define an empty matrix A\n",
    "img_size_example =      #### COMPLETE HERE #####\n",
    "pixel_size_example =    #### COMPLETE HERE #####\n",
    "A_example =             #### COMPLETE HERE #####\n",
    "\n",
    "# Build the forward operator, one column at a time. \n",
    "for i in range(img_size_example):\n",
    "    for j in range(img_size_example):\n",
    "        # Activating a single pixel of the object image\n",
    "        image =         #### COMPLETE HERE #####\n",
    "        image[i,j] =    #### COMPLETE HERE #####\n",
    "        \n",
    "        # Radon transform\n",
    "        sinogram =      #### COMPLETE HERE #####\n",
    "        sinogram =      #### COMPLETE HERE (rescale)#####\n",
    "        \n",
    "        # Concatenating results in matrix A \n",
    "        A_example[:,img_size_example*i+j] = np.reshape(sinogram, (pixel_size_example*total_angles, ))\n",
    "\n",
    "# A matrix visualisation\n",
    "fig, ax = plt.subplots(figsize=(100, 2))\n",
    "ax.imshow(np.transpose(A_example))\n",
    "ax.set_title(\"transpose of A\")\n",
    "ax.set_xlabel(r\"Projection ray $(r, \\theta)$\")\n",
    "ax.set_ylabel(r\"Image pixel $x$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Q: How does the Radon matrix $\\mathbf{A}$ look like? Especially, why is it sparse?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> Testing the resulting matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Check that the forward operator corresponds to Radon transform. For this, complete the code below to compare the sinograms obtained with the radon function and using the matrix-vector product**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phantom = shepp_logan_phantom()\n",
    "phantom = rescale(phantom, scale=(img_size_example/phantom.shape[0]), mode='reflect', multichannel=False)\n",
    "\n",
    "# Radon transform with skimage function\n",
    "radon1 =                      #### COMPLETE HERE #####\n",
    "radon1 = rescale(radon1, scale=(pixel_size_example/radon1.shape[0], 1), mode='reflect', multichannel=False) ##COMPLETE\n",
    "\n",
    "# Radon transform as a matrix-vector product ## COMPLETE\n",
    "f =                           #### COMPLETE HERE #####\n",
    "m =                           #### COMPLETE HERE #####\n",
    "radon2 =                      #### COMPLETE HERE #####\n",
    "\n",
    "# Plots ##COMPLETE THE PLOT\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4.5))\n",
    "ax1.set_title(r\"Sinogram from\" + \"\\nRadon function\")\n",
    "ax1.set_xlabel(r\"Projection angle $\\theta$ (in deg)\")\n",
    "ax1.set_ylabel(r\"Projection position $r$ (in pixels)\")\n",
    "ax1.imshow(radon1, cmap=plt.cm.Greys_r, extent=(0, 180, 0, radon1.shape[0]), aspect='auto')\n",
    "\n",
    "ax2.set_title(r\"Sinogram from\" + \"\\n\" + r\"forward matrix $A$\")\n",
    "ax2.set_xlabel(r\"Projection angle $\\theta$ (in deg)\")\n",
    "ax2.set_ylabel(r\"Projection position $r$ (in pixels)\")\n",
    "ax2.imshow(radon2, cmap=plt.cm.Greys_r, extent=(0, 180, 0, radon2.shape[0]), aspect='auto')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> Inverse matrix\n",
    "We now aim at retrieving $\\textbf{f}$ from the measurements $\\textbf{m}$. \n",
    "    \n",
    "A basic idea could be to invert the matrix $\\textbf{A}$. However, the matrix is not square and is badly [conditioned](https://en.wikipedia.org/wiki/Condition_number). Therefore, we will consider the Moore-Penrose [pseudo inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse), the most widely known generalization of the inverse matrix.\n",
    "     \n",
    "The pseudo-inverse of a matrix $\\textbf{A}$, denoted $\\textbf{A}^\\dagger$, is the matrix such that $\\tilde{\\textbf{f}} = \\textbf{A}^\\dagger \\textbf{m}$ solves the problem of inverting $\\textbf{m} = \\textbf{Af}$ in the least squares sense.\n",
    "\n",
    "<img src=\"fig/ill_pinv.png\" alt=\"m and A\" style=\"width: 100%;\"/>    \n",
    "    \n",
    "<!-- It can be shown that if $Q_1 \\Sigma Q_2^T = A$ is the singular value decomposition of A, then $A^\\dagger = Q_2 \\Sigma^+ Q_1^T$, where $Q_{1,2}$ are orthogonal matrices, $\\Sigma$ is a diagonal matrix consisting of $A$’s so-called singular values (followed typically by zeros), and $\\Sigma^\\dagger$ is the diagonal matrix consisting of the reciprocals of $A$’s singular values (again, followed by zeros). [1]\n",
    "\n",
    "[1] G. Strang, Linear Algebra and Its Applications, 2nd Ed., Orlando, FL, Academic Press, Inc., 1980, pp. 139-142. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Complete the code to reconstruct $\\textbf{f}$ from $\\textbf{m}$ by computing the least square solution using two different methods that you will compare**</font>\n",
    "\n",
    "<font color='green'>**Help: First, use the pseudo inverse of $\\textbf{A}$, which can be computed using this sciPy [function](https://docs.scipy.org/doc//numpy-1.14.1/reference/generated/numpy.linalg.pinv.html).**</font>\n",
    "\n",
    "<font color='green'>**Help: Next, use a linear solver to invert the system $\\textbf{m} = \\textbf{Af}$. See for instance, this sciPy [function](https://docs.scipy.org/doc/scipy-0.16.1/reference/generated/scipy.linalg.lstsq.html)**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the computed sinogram\n",
    "sinogram = np.reshape(radon1, (-1, 1))\n",
    "\n",
    "# Compute the pseudoinverse\n",
    "t0 = time.perf_counter()\n",
    "pinv =                           #### COMPLETE HERE #####\n",
    "t0 = time.perf_counter() - t0\n",
    "print(t0)\n",
    "\n",
    "# Reconstruct with pseudoinverse \n",
    "t1 = time.perf_counter()\n",
    "rec_pi =                           #### COMPLETE HERE #####\n",
    "t1 = time.perf_counter() - t1\n",
    "\n",
    "# Reconstruct with a linear solver\n",
    "t2 = time.perf_counter()\n",
    "rec_solv =                           #### COMPLETE HERE #####\n",
    "t2 = time.perf_counter() - t2\n",
    "\n",
    "# Display results\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4.5))\n",
    "ax1.set_title(\"Ground Truth\")\n",
    "ax1.imshow(phantom, cmap=plt.cm.Greys_r, extent=(0, 180, 0, phantom.shape[0]), aspect='auto')\n",
    "\n",
    "ax2.set_title(f'Recon with pseudoinverse \\n Time : {t0:.3f} + {t1:.3f} s')\n",
    "ax2.imshow(rec_pi, cmap=plt.cm.Greys_r, extent=(0, 180, 0, rec_pi.shape[0]), aspect='auto')\n",
    "\n",
    "ax3.set_title(f'Recon with solver \\n Time : {t2:.3f} s')\n",
    "ax3.imshow(rec_solv, cmap=plt.cm.Greys_r, extent=(0, 180, 0, rec_solv.shape[0]), aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Which approach is faster? When should we use one or the other?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **2 - Limited-angle acquisition and reconstruction**\n",
    "\n",
    "## <span style=\"color:brown\"> Forward operator \n",
    "\n",
    "To limit the acquisition time, it may be desirable to acquire only some of the projection rays (i.e, reduce the number of projection angles or detector pixels). We investigate here the behaviour of the pseudoinverse reconstruction for limite-angle acquisition. The limited-angle forward operator can be obtained by discarding some of the rows of the full forward operator, as illustrated below.\n",
    "    \n",
    "<img src=\"fig/Explain_a_reduced.PNG\" alt=\"m and A\" style=\"width: 80%;\"/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we will consider images of size $64 \\times 64$. In the code below, we will consider the acquisition of 20 projection angles only. We first load the full forward matrix that has been pre-computed. Only some of the rows of the full are kept to build the limited-angle forward operator 'A_reduced'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Q: What are the dimensions of the limited-angle forward operator A_reduced? Complete the code below**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "data_root = './data/'\n",
    "saved_data = data_root + 'matrices/'\n",
    "\n",
    "# Load forward matrix with full angle data\n",
    "radon_matrix_path = saved_data + 'Q{}_D{}.mat'.format(img_size, pixel_size)\n",
    "H = sio.loadmat(radon_matrix_path)\n",
    "A = H.get(\"A\")\n",
    "A = np.array(A)\n",
    "A = torch.from_numpy(A)\n",
    "A = A.type(torch.FloatTensor)\n",
    "\n",
    "# Compute the reduced forward matrix\n",
    "nbAngles = 20\n",
    "Areduced = model_radon.radonSpecifyAngles(A, model_radon.generateAngles(nbAngles))\n",
    "Areduced = Areduced.type(torch.FloatTensor)\n",
    "\n",
    "# print dimension of forward operators\n",
    "                          #### COMPLETE HERE #####\n",
    "                          #### COMPLETE HERE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Q: Are A and A_reduced still numpy arrays? What is their type? Why changing?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of forward matrices\n",
    "                          #### COMPLETE HERE #####\n",
    "                          #### COMPLETE HERE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Reconstruction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compare the quality of the reconstructions obtained from full-angle and limited-angle measurements.\n",
    "\n",
    "<font color='blue'> **Q: Complete the code below to compute the sinograms with both forward operators**</font>\n",
    "\n",
    "<font color='green'> **Help: You can use the torch [matrix-vector mutiplication](https://pytorch.org/docs/stable/generated/torch.mv.html)** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "im = Image.open(\"fig/image.png\")\n",
    "im = ImageOps.grayscale(im)\n",
    "\n",
    "# Preprocess the image\n",
    "im_array = np.asarray(im)\n",
    "im_array = im_array.astype(np.float32)\n",
    "im_array = 2*(im_array)/255 - np.ones([64,64])\n",
    "\n",
    "# Conversion of object image to torch tenser\n",
    "f = torch.from_numpy(im_array)\n",
    "f = f.view(1,img_size**2);\n",
    "f = f.t()\n",
    "f = f.type(torch.FloatTensor)\n",
    "\n",
    "# Simulate the measurements with full angle and limited angle configurations\n",
    "m_reduced =                           #### COMPLETE HERE #####\n",
    "m_perfect =                           #### COMPLETE HERE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Q: Reconstruct the image with the pseudoinverse and plot them.**</font>\n",
    "\n",
    "<font color='green'> **Help: You can use the torch [pseudo inverse](https://pytorch.org/docs/stable/generated/torch.pinverse.html).** </font>\n",
    "\n",
    "<font color='green'> **Warning: Play with the 'rcond' parameter to compute the pseudo inverse of 'Areduced'**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pseudo-inverse\n",
    "pinvA =                           #### COMPLETE HERE #####\n",
    "pinvAreduced =                    #### COMPLETE HERE #####\n",
    "\n",
    "# Full-angle reconstruction\n",
    "f_perfect =                       #### COMPLETE HERE #####\n",
    "f_perfect_array = model_radon.vector2matrix(f_perfect, [img_size,img_size]) # Resize to a 2D shape\n",
    "f_perfect_array = np.transpose(f_perfect_array)\n",
    "\n",
    "# Limited angle reconstruction\n",
    "f_reconstruct =                   #### COMPLETE HERE #####\n",
    "f_reconstruct_array = model_radon.vector2matrix(f_reconstruct, [img_size,img_size]) # Resize to a 2D shape\n",
    "f_reconstruct_array = np.transpose(f_reconstruct_array)\n",
    "\n",
    "#============ Display results ================================\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(22, 4.5))\n",
    "\n",
    "ax1.set_title(\"Input image\")\n",
    "pcm1 = ax1.imshow(im_array, cmap='gray')\n",
    "ax1.set_axis_off()\n",
    "\n",
    "ax2.set_title(\"Sinogram\")\n",
    "ax2.set_xlabel(\"Projection angle (deg)\")\n",
    "ax2.set_ylabel(\"Projection position (pixels)\")\n",
    "m_perfect_array = model_radon.vector2matrix(m_perfect, [total_angles,pixel_size])\n",
    "pcm2 = ax2.matshow(m_perfect_array, cmap='gray')\n",
    "\n",
    "ax3.set_title(\"Reconstructed image with 181 angles measured\")\n",
    "pcm3 = ax3.matshow(f_perfect_array, cmap='gray')\n",
    "ax3.set_axis_off()\n",
    "\n",
    "fig.colorbar(pcm1,ax=ax1)\n",
    "fig.colorbar(pcm2,ax=ax2)\n",
    "fig.colorbar(pcm3,ax=ax3)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig2, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(25, 4.5))\n",
    "\n",
    "ax1.set_title(\"Input image\")\n",
    "pcm1 = ax1.matshow(im_array, cmap='gray')\n",
    "ax1.set_axis_off()\n",
    "\n",
    "ax2.set_title(\"Sinogram\")\n",
    "ax2.set_xlabel(\"Projection angle (deg)\")\n",
    "ax2.set_ylabel(\"Projection position (pixels)\")\n",
    "m_array = model_radon.vector2matrix(m_reduced, [nbAngles,pixel_size])\n",
    "pcm2 = ax2.matshow(m_array, cmap='gray')\n",
    "\n",
    "ax3.set_title(\"Reconstructed image with 20 angles measured\")\n",
    "pcm3 = ax3.matshow(f_reconstruct_array, cmap='gray')\n",
    "ax3.set_axis_off()\n",
    "\n",
    "fig2.colorbar(pcm1,ax=ax1)\n",
    "fig2.colorbar(pcm2,ax=ax2)\n",
    "fig2.colorbar(pcm3,ax=ax3)\n",
    "fig2.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Q: How do the full-angle and limited-angle reconstructions compare?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Influence of the number of angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have obseved the degradation of the image quality due to limited-angle measurements. We will now investigate the influence of the numbers of projection angles.\n",
    "\n",
    "<font color='blue'> **Complete and run the code below. NB: this takes about 60 seconds**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listAngles = [5, 10, 20, 40, 60, 100]\n",
    "\n",
    "##PERFORM THE WHOLE LOOP\n",
    "for ang in listAngles:\n",
    "    # Compute Areduced\n",
    "    Areduced = model_radon.radonSpecifyAngles(A, model_radon.generateAngles(ang))\n",
    "    Areduced = Areduced.type(torch.FloatTensor)\n",
    "    \n",
    "    # Compute the pseudoinverse    \n",
    "    pinvAreduced =                           #### COMPLETE HERE #####\n",
    "    \n",
    "    # Simulate the measurements\n",
    "    m = torch.mv(Areduced,f[:,0])\n",
    "    \n",
    "    # Reconstruct the image\n",
    "    f_reconstruct = torch.mv(pinvAreduced,m)\n",
    "    \n",
    "    # Reshape into a 2D mage\n",
    "    f_reconstruct_array = model_radon.vector2matrix(f_reconstruct, [img_size,img_size])\n",
    "    f_reconstruct_array = np.transpose(f_reconstruct_array)\n",
    "    \n",
    "    # Display results\n",
    "    plt.imshow(f_reconstruct_array, cmap='gray')\n",
    "    plt.title(f'Reconstruction w {ang} angles measured')\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Q: What is the minimum number of measurement angles that can be reconstucted using the pseudo-inverse matrix?** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **3 - Deep image reconstruction**\n",
    "## <span style=\"color:brown\"> Framework\n",
    "    \n",
    "Deep image reconstruction aims to design a non-linear mapping (i.e., neural network) $\\mathcal{G}_{\\omega}$ such that \n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{G}_{\\omega}(\\mathbf{m}) \\approx \\mathbf{f},\n",
    "\\label{eq:mapping} \\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\omega$represents the parameters of the network. The parameters are optimized during the training phase to minimize the cost function\n",
    "\n",
    "\\begin{equation*}\n",
    "\\omega^* = \\underset{\\omega}{\\text{arg min}} \\sum_{\\ell=0}^{S-1} \\| \\mathcal{G}_\\omega(\\mathbf{m}^{(\\ell)}) - \\mathbf{f}^{(\\ell)}\\|^2_2 + \\mathcal{R}(\\omega),\n",
    "\\label{eq:fn} \\tag{2}\n",
    "\\end{equation*}\n",
    "where $(\\mathbf{m}^{(\\ell)} , \\mathbf{f}^{(\\ell)})$, $0 \\le \\ell \\le L-1$ are the measurement-image pairs of the training database, and $\\mathcal{R}$ is a regularization function that stabilizes training. For this study we will consider $\\mathcal{R}(.) = \\alpha \\|.\\|^2_2$, where $\\alpha$ is a positive constant that will impact how important $\\mathcal{R}$ is with respect to the rest of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Network architecture\n",
    "\n",
    "As illustrated below, we choose to map the sinogram $\\mathbf{m}$ into the image domain (see $\\tilde{\\mathbf{f}}$) to benefit convolutional layers are particularly powerful for image denoising and artefact correction.\n",
    "\n",
    "<img src=\"fig/network.png\" alt=\"m and A\" style=\"width: 70%;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Why do we map the sinogram into the image domain before applying convolutional layers? Why do we use the Moore-Penrose pseudo-inverse rather than learning this mapping?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyrit.learning.model_Had_DCAN import Weight_Decay_Loss\n",
    "from spyrit.learning.nets import train_model\n",
    "\n",
    "net_arch = 2\n",
    "regularisation = 1e-7\n",
    "\n",
    "num_epochs=3\n",
    "batch_size=256\n",
    "reg=1e-7\n",
    "lr=1e-3\n",
    "step_size=20\n",
    "gamma=0.2\n",
    "checkpoint_model=\"\"\n",
    "checkpoint_interval=0\n",
    "data_root = './data/'\n",
    "model_root='./models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.functional.to_grayscale,\n",
    "     transforms.Resize((img_size, img_size)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=data_root+\"train\", transform=transform)\n",
    "trainloader = \\\n",
    "    torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=False)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=data_root+\"test\", transform=transform)\n",
    "testloader = \\\n",
    "    torch.utils.data.DataLoader(testset, batch_size=batch_size,shuffle=False)\n",
    "\n",
    "dataloaders = {'train': trainloader, 'val': testloader}\n",
    "inputs, labels = next(iter(dataloaders['val']))\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "im_tensor = torch.from_numpy(im_array)\n",
    "m = torch.mv(Areduced,f[:,0])\n",
    "test_batch = 1\n",
    "color = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: What is the shape of the database 'input' variable? What does each dimension correspond to?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                          #### COMPLETE HERE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Explain the line 'transforms.Normalize([0.5], [0.5])'. Why is it important?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Training a reconstruction network from scratch (for 3 epochs)\n",
    "\n",
    "<font color='blue'>**Q: Complete the code to compute the forward operator 'Areduced' (20 projections) and the corresponding pseudo-inverse matrix 'pinvAreduced'. NB: traing takes about two minutes**</font>\n",
    "\n",
    "<font color='green'>**Help: you have already done this in some of previous cells. The point here is to understand how the pseudo inverse is plugged into the deep reconstruction network.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_amt = 4\n",
    "nbAngles = 20\n",
    "\n",
    "###################  TO BE COMPLETED\n",
    "Areduced =                           #### COMPLETE HERE #####\n",
    "Areduced =                           #### COMPLETE HERE #####\n",
    "pinvAreduced =                       #### COMPLETE HERE #####\n",
    "\n",
    "\n",
    "model = model_radon.compNet(img_size, pixel_size, nbAngles, A = Areduced, pinvA = pinvAreduced, variant=net_arch)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "loss = nn.MSELoss();\n",
    "criterion = Weight_Decay_Loss(loss);\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr);\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "model, train_info = train_model(model, criterion, \\\n",
    "        optimizer, scheduler, dataloaders, device, model_root, num_epochs=num_epochs,\\\n",
    "        disp=True, do_checkpoint=checkpoint_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Complete the code below to compare the ground-truth, pseudo-inverse, and the deep solutions.**</font>\n",
    "\n",
    "<font color='green'>**Tip1: Remember the dimension of 'input' from the database.**</font>\n",
    "\n",
    "<font color='green'>**Tip2: Torch tensors may be in the memory of the GPU**</font>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(test_amt):\n",
    "    \n",
    "    # Random image in STL10\n",
    "    i_test = np.random.randint(0, inputs.shape[0])\n",
    "    \n",
    "    # Plots\n",
    "    fig, axs = plt.subplots(1, 3, figsize =(20,10))\n",
    "    fig.suptitle('', fontsize=16)\n",
    "    \n",
    "    ax = axs[0]\n",
    "    ax.set_title(\"Ground-truth\")\n",
    "    aff = ax.imshow(                          #### COMPLETE HERE #####\n",
    "    fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "\n",
    "    ax = axs[1]\n",
    "    ax.set_title(\"Pseudo-inverse \")\n",
    "    rec = model.evaluate_fcl(inputs)\n",
    "    aff = ax.imshow(                          #### COMPLETE HERE #####\n",
    "    fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "\n",
    "    ax = axs[2]\n",
    "    ax.set_title(\"Deep network\")\n",
    "    rec = model.evaluate(inputs)\n",
    "    aff = ax.imshow(                          #### COMPLETE HERE #####\n",
    "    fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: How do the pseudo inverse solution and network output compare?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Testing a trained model\n",
    "    \n",
    "Here we load networks that we have already trained for larger numbers of epochs\n",
    "    \n",
    "<font color='blue'> **Run the code below**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_types = ['c0mp', 'comp','pinv', 'free']\n",
    "net_arch = 2\n",
    "num_epoch = 100\n",
    "list_angles = np.array([20, 40, 60])\n",
    "learning_rate = 1e-3\n",
    "step_size = 10\n",
    "gamma = 0.5\n",
    "batch_size = 1000\n",
    "regularisation = 1e-7\n",
    "\n",
    "test_amt = 4\n",
    "nbAngles = 20\n",
    "inputs, labels = next(iter(dataloaders['val']))\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Network filename\n",
    "suffix = '_Q_{}_D_{}_T_{}_epo_{}_lr_{}_sss_{}_sdr_{}_bs_{}_reg_{}'.format(\\\n",
    "               img_size, pixel_size, nbAngles, num_epoch, learning_rate, step_size,\\\n",
    "               gamma, batch_size, regularisation)\n",
    "title = data_root + 'NET_'+ net_types[net_arch] + suffix\n",
    "\n",
    "# Loading the model\n",
    "model = model_radon.compNet(img_size, pixel_size, nbAngles, variant=net_arch)\n",
    "model = model.to(device)\n",
    "model_out_path = \"{}.pth\".format(title)\n",
    "model.load_state_dict(torch.load(model_out_path, map_location=torch.device('cpu')))\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Q: How do the arguments of the 'compNet' model relate to the network architecture depicted above?**</font>\n",
    "\n",
    "<font color='green'>**Tip: Looking at the next section may help to answer this question**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <font color='blue'> **Run the code below**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(test_amt):\n",
    "    # Choosing random image in STL10\n",
    "    i_test = np.random.randint(0, inputs.shape[0])\n",
    "    \n",
    "    # Plots\n",
    "    fig, axs = plt.subplots(1, 3, figsize =(20,10))\n",
    "    fig.suptitle('', fontsize=16)\n",
    "    \n",
    "    ax = axs[0]\n",
    "    ax.set_title(\"Ground-truth\")\n",
    "    aff = ax.imshow(inputs[i_test, 0, :, :].cpu(), cmap='gray')\n",
    "    fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "\n",
    "    ax = axs[1]\n",
    "    ax.set_title(\"Pseudo-inverse\")\n",
    "    rec = model.evaluate_fcl(inputs)\n",
    "    aff = ax.imshow(rec[i_test, 0, :, :].cpu(), cmap='gray')\n",
    "    fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "\n",
    "    ax = axs[2]\n",
    "    ax.set_title(\"Deep network\")\n",
    "    rec = model.evaluate(inputs)\n",
    "    aff = ax.imshow(rec[i_test, 0, :, :].cpu(), cmap='gray')\n",
    "    fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: What is the impact of training a deep neural network until convergence?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Influence of the number of projection angles\n",
    "    \n",
    "Here we load networks that we have already trained for different numbers of projection angles\n",
    "    \n",
    "<font color='blue'> **Complete and run the code below. You can also try to reconstruct different images in the batch**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(list_angles.size):\n",
    "    \n",
    "    print(\"Acquisition of \" + list_angles[index].astype(str) + \" angles\")\n",
    "    \n",
    "    # Networks filenameN\n",
    "    suffix = '_Q_{}_D_{}_T_{}_epo_{}_lr_{}_sss_{}_sdr_{}_bs_{}_reg_{}'.format(\\\n",
    "                   img_size, pixel_size, list_angles[index], num_epoch, learning_rate, step_size,\\\n",
    "                   gamma, batch_size, regularisation)\n",
    "    title = data_root + 'NET_'+ net_types[net_arch] + suffix\n",
    "    \n",
    "    # loading model\n",
    "    model = model_radon.compNet(img_size, pixel_size, list_angles[index], variant=net_arch)\n",
    "    model = model.to(device)\n",
    "    model_out_path = \"{}.pth\".format(title)\n",
    "    model.load_state_dict(torch.load(model_out_path, map_location=torch.device('cpu'))) \n",
    "    model_bis = model;\n",
    "    \n",
    "    #=======================Plots========================================================\n",
    "    fig, axs = plt.subplots(1, 4, figsize =(20,10))\n",
    "    fig.suptitle('', fontsize=16)\n",
    "    \n",
    "    ax = axs[0]\n",
    "    ax.set_title(\"Sinogram\")\n",
    "    rec = model.forward_acquire(inputs, test_batch, color, img_size, img_size)\n",
    "    rec_array = model_radon.vector2matrix(rec[0, :, 0, 0].cpu(), [list_angles[index], 64])\n",
    "    aff = ax.imshow(rec_array, cmap='gray', aspect='auto')\n",
    "    #fig.colorbar(aff, ax=ax)\n",
    "    \n",
    "    ax = axs[1]\n",
    "    ax.set_title(\"Ground-truth\")\n",
    "    aff = ax.imshow(inputs[test_batch, 0, :, :].cpu(), cmap='gray') # Complete\n",
    "    #fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "\n",
    "    ax = axs[2]\n",
    "    ax.set_title(\"Pseudo-inverse\")\n",
    "    rec = model.evaluate_fcl(inputs)\n",
    "    aff = ax.imshow(rec[0, 0, :, :].cpu(), cmap='gray') \n",
    "    #fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "\n",
    "    ax = axs[3]\n",
    "    ax.set_title(\"Deep network\")\n",
    "    rec = model.evaluate(inputs)\n",
    "    aff = ax.imshow(rec[0, 0, :, :].cpu(), cmap='gray') \n",
    "    #fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Conclude on the limitations of deep-neural reconstructors.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **Conclusion**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this hands-on session, you should be able to\n",
    "* Understand the importance of modeling the forward operator of an inverse problem.\n",
    "* Reconstruct an image using a linear reconstructor (e.g., Moore-Penrose pseudo-inverse).\n",
    "* Understand the limitations of linear reconstruction methods for undersampled data (e.g., limited-angle tomography).\n",
    "* Reconstruct an image using a non-linear reconstructor (e.g., deep convolutional neural networks). \n",
    "* Integrate linear reconstructors into a deep-learning method.\n",
    "* Understand the impact of the training parameters (e.g., the choice of the number of epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
