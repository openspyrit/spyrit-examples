{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **Deep image reconstruction using the PyTorch and SPyRiT packages**\n",
    "\n",
    "## <span style=\"color:brown\"> **With application to limited-angle computed tomography**\n",
    "\n",
    "*PyTorch 1.10.1; SPyRiT 2.0.0*\n",
    "\n",
    "Authors (version 2023): L Amador, E Chen, N Ducros, H-J Ling, K Mom, J Puig, T Grenier, E Saillard,      \n",
    "Authors (version 2021): N Ducros, T Leuliet, A Lorente Mur, Louise Friot-Giroux\n",
    "    \n",
    "Contact: *nicolas.ducros@creatis.insa-lyon.fr*\n",
    "    \n",
    "##  <span style=\"color:brown\"> Data download (just once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isfile(\"data.zip\"):\n",
    "    !wget \"https://www.creatis.insa-lyon.fr/~ducros/spyritexamples/2023_DLMIS/data.zip\"\n",
    "    !unzip -qq data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  <span style=\"color:brown\"> Installations (just once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openspyrit/spyrit@master\n",
    "!pip install h5py\n",
    "!pip install scikit-image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  <span style=\"color:brown\"> Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as lin\n",
    "from PIL import Image, ImageOps\n",
    "from model_radon import radonSpecifyAngles, vector2matrix\n",
    "from skimage.data import shepp_logan_phantom\n",
    "from skimage.transform import radon, rescale\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- Acquisition\n",
    "img_size = 64 # image size\n",
    "pixel_size = 64 #Number of pixels of the sensor\n",
    "\n",
    "#- Using CPU or GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **1 - Computed Tomography (CT) and Radon operator**\n",
    "\n",
    "## Computed Tomography (CT)\n",
    "Computed tomography (CT) is an imaging modality that reconstructs 2D or 3D objects from attenuation measurements. CT is a technique used in non-destructive inspection but most notably in medical imaging, where attenuation allows the type of tissue (e.g., bone, soft tissue) and structures (e.g., tumors) to be identified. The image formation process can be modelled by the Radon transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Radon transform\n",
    "The [Radon transform](https://en.wikipedia.org/wiki/Radon_transform) is an integral transform that returns line integrals over hyperplanes (e.g., along lines for a 2D object). We illustrate this process in a discrete setting below. We consider the projection of a discrete object image $\\mathbf{x}$ along a projection ray $(r_j, \\theta_k)$, where $\\{r_j\\}$ is the detector pixel locations and $\\theta_k$ the projection angle. The integral  measured for all detector pixels under all projection views is known as the '[sinogram](https://en.wikipedia.org/wiki/Radon_transform#/media/File:Radon_transform_sinogram.gif)'. \n",
    "\n",
    "<img src=\"fig/tomo.png\" alt=\"Projections schem\" style=\"width: 80%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: How does the sinogram of a point object (i.e., an image with only one nonzero pixel) look like? Complete the code below**\n",
    "    \n",
    "<font color='green'>**Help: Create a (Q x Q) image with only one pixel set to 1. Set all the other pixels to 0.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector with acquired angles\n",
    "theta = np.linspace(0., 180., 181)\n",
    "\n",
    "# Image with a single pixel set to one (point response function)\n",
    "example = np.zeros((XXX, XXX)) #COMPLETE\n",
    "example[5,5] =                 #COMPLETE\n",
    "\n",
    "# Sinogramm (CT measurements from object image)\n",
    "sinogram = radon(example, theta, circle=False)\n",
    "sinogram = rescale(sinogram, scale=(pixel_size/sinogram.shape[0],1), mode='reflect')\n",
    "\n",
    "# Plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4.5))\n",
    "ax1.set_title(\"Object image\")\n",
    "ax1.imshow(example, cmap=plt.cm.Greys_r)\n",
    "\n",
    "ax2.set_title(\"Sinogram\")\n",
    "ax2.set_xlabel(\"Projection angle (deg)\")\n",
    "ax2.set_ylabel(\"Dectector pixel\")\n",
    "ax2.imshow(sinogram, cmap=plt.cm.Greys_r,\n",
    "           extent=(0, 180, 0, sinogram.shape[0]), aspect='auto')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Explain the 'sinogram' terminology.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **A: The point response of the Radon transform is a sine. Therefore, any sinogram can be seen as the weighted sum of many sines, each of them originating from one of the pixels in the object image.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> The Radon operator\n",
    "The Radon transform is a linear operator. Therefore, the sinogram $\\textbf{m}$ can be simply obtained as the matrix vector product $\\textbf{m} = \\textbf{Af}$, where $\\textbf{A}$ represents the discrete Radon (forward) operator and $\\textbf{f}$ is the object image. Both $\\textbf{m}$ and $\\textbf{f}$ are column vectors; $\\textbf{m}$ contains all the measurements, $\\textbf{f}$ all the image pixels. $\\textbf{A}$ is a matrix, whose dimensions match with the dimentions of $\\textbf{m}$ and $\\textbf{f}$.\n",
    "\n",
    "This is illustrated below. \n",
    "    \n",
    "<img src=\"fig/def_var.png\" alt=\"m and A\" style=\"width: 100%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> Creating the operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective here is to create this forward operator $A$ for a toy example of images of size $16\\times 16$. \n",
    "\n",
    "<font color='blue'>**Q: Based on the explanation above, complete the code below to create the matrix 'A_example'**</font>.\n",
    "\n",
    "<font color='green'>**Help: First, determine the size of the Radon matrix**</font>\n",
    "\n",
    "<font color='green'>**Help: Next, generate one column of the Radon matrix at a time using the 'radon' function**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_angles = 181\n",
    "\n",
    "# Define an empty matrix A\n",
    "img_size = 16\n",
    "pixel_size = 16\n",
    "A = np.zeros((pixel_size*total_angles, img_size*img_size)) #COMLPLETE\n",
    "\n",
    "# Build the forward operator, one column at a time. \n",
    "for i in range(img_size):\n",
    "    for j in range(img_size):\n",
    "        # Activating a single pixel of the object image\n",
    "        image =  ##COMPLETE\n",
    "        image[i,j] =  ##COMLPETE\n",
    "        \n",
    "        # Radon transform\n",
    "        sinogram =  ##COMPLETE\n",
    "        sinogram = rescale(sinogram, scale=(pixel_size/sinogram.shape[0],1), mode='reflect')\n",
    "        \n",
    "        # Concatenating results in matrix A \n",
    "        A[:,img_size*i+j] = np.reshape(sinogram, (pixel_size*total_angles, ))\n",
    "\n",
    "# A matrix visualisation\n",
    "fig, ax = plt.subplots(figsize=(100, 2))\n",
    "ax.imshow(np.transpose(A))\n",
    "ax.set_title(\"transpose of A\")\n",
    "ax.set_xlabel(r\"Projection ray $(r, \\theta)$\")\n",
    "ax.set_ylabel(r\"Image pixel $x$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Q: How does the Radon matrix $\\mathbf{A}$ look like? Especially, why is it sparse?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **Each row corresponds to one pixel position in the image domain. Each column corresponds to one projection ray in the sinogram. The matrix is sparse because only few pixels in the object image contribute to the measurement obtained for a given detector pixel under a given projection angle. For instance, at projection angle 0, there are only 16 pixels in the image that contribute to the measurement in each detector pixel, which we can observe with the \"lines\" in the forward matrix.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> Testing the resulting matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Check that the forward operator corresponds to Radon transform. For this, complete the code below to compare the sinograms obtained with the radon function and using the matrix-vector product**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phantom = shepp_logan_phantom()\n",
    "phantom = rescale(phantom, scale=(img_size/phantom.shape[0]), mode='reflect')\n",
    "\n",
    "# Radon transform with skimage function\n",
    "radon1 =  ## COMPLETE\n",
    "radon1 = rescale(radon1, scale=(pixel_size/radon1.shape[0], 1), mode='reflect')\n",
    "\n",
    "# Radon transform as a matrix-vector product ## COMPLETE\n",
    "f = \n",
    "m = \n",
    "radon2 = np.reshape(m, (pixel_size,total_angles)) \n",
    "\n",
    "# Plots ##COMPLETE THE PLOT\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4.5))\n",
    "ax1.set_title(r\"Sinogram from\" + \"\\nRadon function\")\n",
    "ax1.set_xlabel(r\"Projection angle $\\theta$ (in deg)\")\n",
    "ax1.set_ylabel(r\"Projection position $r$ (in pixels)\")\n",
    "ax1.imshow(radon1, cmap=plt.cm.Greys_r, extent=(0, 180, 0, radon1.shape[0]), aspect='auto')\n",
    "\n",
    "ax2.set_title(r\"Sinogram from\" + \"\\n\" + r\"forward matrix $A$\")\n",
    "ax2.set_xlabel(r\"Projection angle $\\theta$ (in deg)\")\n",
    "ax2.set_ylabel(r\"Projection position $r$ (in pixels)\")\n",
    "ax2.imshow(radon2, cmap=plt.cm.Greys_r, extent=(0, 180, 0, radon2.shape[0]), aspect='auto')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:brown\"> Inverse matrix\n",
    "We now aim at retrieving $\\textbf{f}$ from the measurements $\\textbf{m}$. \n",
    "    \n",
    "A basic idea could be to invert the matrix $\\textbf{A}$. However, the matrix is not square and is badly [conditioned](https://en.wikipedia.org/wiki/Condition_number). Therefore, we will consider the Moore-Penrose [pseudo inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse), the most widely known generalization of the inverse matrix.\n",
    "     \n",
    "The pseudo-inverse of a matrix $\\textbf{A}$, denoted $\\textbf{A}^+$, is a matrix that provides a least squares solution to the invertion of the linear system $\\textbf{m} = \\textbf{Af}$. We refer to $\\tilde{\\textbf{f}} = \\textbf{A}^+ \\textbf{m}$ as the pseudo inverse solution.\n",
    "\n",
    "<img src=\"fig/pinv.png\" alt=\"m and A\" style=\"width: 100%;\"/>    \n",
    "    \n",
    "<!-- It can be shown that if $Q_1 \\Sigma Q_2^T = A$ is the singular value decomposition of A, then $A^+ = Q_2 \\Sigma^+ Q_1^T$, where $Q_{1,2}$ are orthogonal matrices, $\\Sigma$ is a diagonal matrix consisting of $A$’s so-called singular values (followed typically by zeros), and $\\Sigma^+$ is the diagonal matrix consisting of the reciprocals of $A$’s singular values (again, followed by zeros). [1]\n",
    "\n",
    "[1] G. Strang, Linear Algebra and Its Applications, 2nd Ed., Orlando, FL, Academic Press, Inc., 1980, pp. 139-142. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Complete the code to reconstruct $\\textbf{f}$ from $\\textbf{m}$ by computing the least square solution using two different methods that you will compare**</font>\n",
    "\n",
    "<font color='green'>**Help: First, use the pseudo inverse of $\\textbf{A}$, which can be computed using this sciPy [function](https://docs.scipy.org/doc//numpy-1.14.1/reference/generated/numpy.linalg.pinv.html).**</font>\n",
    "\n",
    "<font color='green'>**Help: Next, use a linear solver to invert the system $\\textbf{m} = \\textbf{Af}$. See for instance, this sciPy [function](https://docs.scipy.org/doc/scipy-0.16.1/reference/generated/scipy.linalg.lstsq.html)**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the computed sinogram\n",
    "sinogram = np.reshape(radon1, (-1, 1))\n",
    "\n",
    "# Compute the pseudoinverse\n",
    "t0 = time.perf_counter()\n",
    "pinv =  # COMPLETE\n",
    "t0 = time.perf_counter() - t0\n",
    "\n",
    "# Reconstruct with pseudoinverse \n",
    "t1 = time.perf_counter()\n",
    "rec_pi = np.reshape(XXX, (img_size,img_size)) # COMPLETE\n",
    "t1 = time.perf_counter() - t1\n",
    "print(f'Recon with pseudoinverse: {t0:.3f} + {t1:.4f} s')\n",
    "\n",
    "# Reconstruct with a linear solver\n",
    "t2 = time.perf_counter()\n",
    "rec_solv = np.reshape(XXX, (img_size,img_size)) # COMPLETE\n",
    "t2 = time.perf_counter() - t2\n",
    "print(f'Recon with solver: {t2:.3f} s')\n",
    "\n",
    "# Display results\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4.5))\n",
    "ax1.set_title(r\"Ground Truth\")\n",
    "ax1.imshow(phantom, cmap=plt.cm.Greys_r)\n",
    "\n",
    "ax2.set_title(r\"Recon with pseudoinverse\")\n",
    "ax2.imshow(rec_pi, cmap=plt.cm.Greys_r)\n",
    "\n",
    "ax3.set_title(r'Recon with solver')\n",
    "ax3.imshow(rec_solv, cmap=plt.cm.Greys_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Which approach is faster? When should we use one or the other?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**A: reconstruction with the solver is much faster (e.g., ~100 ms) than with the pseudo inverse (e.g., ~800 ms). However, once the pseudo inverse is computed, it can be used to reconstruct new data in a very short time (e.g., few milliseconds), while the solver will be significantly slower (e.g., ~100 ms).**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **2 - Limited-angle acquisition and reconstruction**\n",
    "\n",
    "## <span style=\"color:brown\"> Forward operator \n",
    "\n",
    "To limit the acquisition time, it may be desirable to acquire only some of the projection rays (i.e, reduce the number of projection angles or detector pixels). We investigate here the behaviour of the pseudoinverse reconstruction for limite-angle acquisition. The limited-angle forward operator can be obtained by discarding some of the rows of the full forward operator, as illustrated below.\n",
    "    \n",
    "<img src=\"fig/Explain_a_reduced.PNG\" alt=\"m and A\" style=\"width: 80%;\"/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we will consider images of size $64 \\times 64$. In the code below, we will consider the acquisition of 20 projection angles only. We first load the full forward matrix that has been pre-computed. Only some of the rows of the full forward matrix are kept to build the limited-angle forward operator `A_reduced`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Q: What are the dimensions of the limited-angle forward operator `A_reduced`? Complete the code below**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "data_root = './data/'\n",
    "saved_data = data_root + 'matrices/'\n",
    "\n",
    "img_size = 64 # image size\n",
    "pixel_size = 64 #Number of pixels of the sensor\n",
    "\n",
    "radon_matrix_path = saved_data + 'Q{}_D{}.npy'.format(img_size, pixel_size)\n",
    "A = np.load(radon_matrix_path)\n",
    "A = torch.from_numpy(A)\n",
    "A = A.type(torch.FloatTensor)\n",
    "\n",
    "# Compute the reduced forward matrix\n",
    "nbAngles = 20\n",
    "Areduced = radonSpecifyAngles(A, nbAngles)\n",
    "Areduced = torch.from_numpy(Areduced)\n",
    "Areduced = Areduced.type(torch.FloatTensor)\n",
    "\n",
    "# print dimension of forward operators\n",
    "print(XXX) # COMPLETE\n",
    "print(XXX) # COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**A: A_reduced is of size (20x64,64x64)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Q: Are `A` and `A_reduced` still numpy arrays? What is their type? Why changing?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of forward matrices\n",
    "print(XXX) # COMPLETE\n",
    "print(XXX) # COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**A: We now manipulate them as torch tensors for integrating them into neural networks**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Reconstruction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compare the quality of the reconstructions obtained from full-angle and limited-angle measurements.\n",
    "\n",
    "<font color='blue'> **Q: Complete the code below to compute the sinograms with both forward operators**</font>\n",
    "\n",
    "<font color='green'> **Help: You can use the torch [matrix-vector mutiplication](https://pytorch.org/docs/stable/generated/torch.mv.html)** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "im = Image.open(\"fig/image.png\")\n",
    "im = ImageOps.grayscale(im)\n",
    "\n",
    "# Preprocess the image\n",
    "im_array = np.asarray(im)\n",
    "im_array = im_array.astype(np.float32)\n",
    "im_array = 2*(im_array)/255 - np.ones([64,64])\n",
    "\n",
    "# Conversion of object image to torch tenser\n",
    "f = torch.from_numpy(im_array)\n",
    "f = f.view(1,img_size**2);\n",
    "f = f.t()\n",
    "f = f.type(torch.FloatTensor)\n",
    "\n",
    "# Simulate the measurements with full angle and limited angle configurations\n",
    "m_reduced =  ##COMPLETE\n",
    "m_perfect =  ##COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Q: Reconstruct the image with the pseudoinverse and plot them.**</font>\n",
    "\n",
    "<font color='green'> **Help: You can use the torch [pseudo inverse](https://pytorch.org/docs/stable/generated/torch.pinverse.html).** </font>\n",
    "\n",
    "<font color='green'> **Warning: Play with the 'rcond' parameter to compute the pseudo inverse of 'Areduced'**</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pseudo-inverse\n",
    "pinvA =          ##COMPLETE\n",
    "pinvAreduced =  ##COMPLETE\n",
    "\n",
    "# Full-angle reconstruction\n",
    "f_perfect =  ##COMPLETE #should be a 1D vector here\n",
    "f_perfect_array = vector2matrix(f_perfect, [img_size,img_size]) # Resize to a 2D shape\n",
    "f_perfect_array = np.transpose(f_perfect_array)\n",
    "\n",
    "# Limited angle reconstruction\n",
    "f_reconstruct =  ##COMPLETE\n",
    "f_reconstruct_array = vector2matrix(f_reconstruct, [img_size,img_size]) # Resize to a 2D shape\n",
    "f_reconstruct_array = np.transpose(f_reconstruct_array)\n",
    "\n",
    "#============ Display results ================================\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(22, 4.5))\n",
    "\n",
    "ax1.set_title(\"Input image\")\n",
    "pcm1 = ax1.imshow(im_array, cmap='gray')\n",
    "ax1.set_axis_off()\n",
    "\n",
    "ax2.set_title(\"Sinogram\")\n",
    "ax2.set_xlabel(\"Projection angle (deg)\")\n",
    "ax2.set_ylabel(\"Projection position (pixels)\")\n",
    "m_perfect_array = vector2matrix(m_perfect, [total_angles,pixel_size])\n",
    "pcm2 = ax2.matshow(m_perfect_array, cmap='gray')\n",
    "\n",
    "ax3.set_title(\"Reconstructed image with 181 angles measured\")\n",
    "pcm3 = ax3.matshow(f_perfect_array, cmap='gray')\n",
    "ax3.set_axis_off()\n",
    "\n",
    "fig.colorbar(pcm1,ax=ax1)\n",
    "fig.colorbar(pcm2,ax=ax2)\n",
    "fig.colorbar(pcm3,ax=ax3)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig2, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(25, 4.5))\n",
    "\n",
    "ax1.set_title(\"Input image\")\n",
    "pcm1 = ax1.matshow(im_array, cmap='gray')\n",
    "ax1.set_axis_off()\n",
    "\n",
    "ax2.set_title(\"Sinogram\")\n",
    "ax2.set_xlabel(\"Projection angle (deg)\")\n",
    "ax2.set_ylabel(\"Projection position (pixels)\")\n",
    "m_array = vector2matrix(m_reduced, [nbAngles,pixel_size])\n",
    "pcm2 = ax2.matshow(m_array, cmap='gray')\n",
    "\n",
    "ax3.set_title(\"Reconstructed image with 20 angles measured\")\n",
    "pcm3 = ax3.matshow(f_reconstruct_array, cmap='gray')\n",
    "ax3.set_axis_off()\n",
    "\n",
    "fig2.colorbar(pcm1,ax=ax1)\n",
    "fig2.colorbar(pcm2,ax=ax2)\n",
    "fig2.colorbar(pcm3,ax=ax3)\n",
    "fig2.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Q: How do the full-angle and limited-angle reconstructions compare?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**A: Many artefacts are present; most details are lost. The pseudo-inverse matrix leads to poor image quality for limited-angle reconstruction**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Influence of the number of angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed the degradation of the image quality due to limited-angle measurements. We will now investigate the influence of the numbers of projection angles.\n",
    "\n",
    "<font color='blue'> **Complete and run the code below. NB: this takes about 60 seconds**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listAngles = [5, 10, 20, 40, 60, 100]\n",
    "\n",
    "##PERFORM THE WHOLE LOOP\n",
    "for ang in listAngles:\n",
    "\n",
    "    Areduced = radonSpecifyAngles(A, ang)\n",
    "    Areduced = torch.from_numpy(Areduced)\n",
    "    Areduced = Areduced.type(torch.FloatTensor)\n",
    "    \n",
    "    # Compute the pseudoinverse    \n",
    "    pinvAreduced =  # COMPLETE\n",
    "    \n",
    "    # Simulate the measurements\n",
    "    m = torch.mv(Areduced,f[:,0])\n",
    "    \n",
    "    # Reconstruct the image\n",
    "    f_reconstruct = torch.mv(pinvAreduced,m)\n",
    "    \n",
    "    # Reshape into a 2D mage\n",
    "    f_reconstruct_array = vector2matrix(f_reconstruct, [img_size,img_size])\n",
    "    f_reconstruct_array = np.transpose(f_reconstruct_array)\n",
    "    \n",
    "    # Display results\n",
    "    plt.imshow(f_reconstruct_array, cmap='gray')\n",
    "    plt.title(f'Reconstruction w {ang} angles measured')\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Q: What is the minimum number of measurement angles that can be reconstucted using the pseudo-inverse matrix?** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**A: More than 60 angles are necessary. NB: the computation time of the pseudo inverse matrix increases dramatically with the number of angles, which is due the [computational compexity of singular value decomposition](https://stackoverflow.com/questions/58191604/what-is-the-time-complexity-of-the-pseudo-inverse-in-pytorch-i-e-torch-pinvers)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **3 - Deep image reconstruction**\n",
    "## <span style=\"color:brown\"> Framework\n",
    "    \n",
    "Deep image reconstruction aims to design a non-linear mapping (i.e., neural network) $\\mathcal{G}_{\\omega}$ such that \n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{G}_{\\omega}(\\mathbf{m}) \\approx \\mathbf{f},\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\omega$represents the parameters of the network. The parameters are optimized during the training phase to minimize the cost function\n",
    "\n",
    "\\begin{equation*}\n",
    "\\omega^* = \\underset{\\omega}{\\text{arg min}} \\sum_{\\ell=0}^{S-1} \\| \\mathcal{G}_\\omega(\\mathbf{m}^{(\\ell)}) - \\mathbf{f}^{(\\ell)}\\|^2_2 + \\mathcal{R}(\\omega),\n",
    "\\end{equation*}\n",
    "where $\\{\\mathbf{m}^{(\\ell)} , \\mathbf{f}^{(\\ell)}\\}$, $0 \\le \\ell \\le L-1$ are the measurement-image pairs of the training database, and $\\mathcal{R}$ is a regularization function that stabilizes training. For this study we will consider $\\mathcal{R}(.) = \\alpha \\|.\\|^2_2$, where $\\alpha$ is a positive constant that will impact how important $\\mathcal{R}$ is with respect to the rest of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Network architecture\n",
    "\n",
    "As illustrated below, we choose to map the sinogram $\\mathbf{m}$ into the image domain (see $\\tilde{\\mathbf{f}}$) to benefit convolutional layers are particularly powerful for image denoising and artefact correction.\n",
    "\n",
    "<img src=\"fig/network.png\" alt=\"m and A\" style=\"width: 100%;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Why do we map the sinogram into the image domain before applying convolutional layers? Why do we use the Moore-Penrose pseudo-inverse rather than learning this mapping?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**A: First, convolutional layers have shown great success in exploiting spacial redundancies of natural images, so we would like to apply these layers in the image domain. While several studies have shown that convolutional layers can be used on sinograms too, image-domain processing is a safe choice. Second, learning the measurement to image domain mapping, would require learning a lot of parameters (i.e, $D\\times \\Theta \\times, Q^2$ parameters) and the resulting network may be more sensitive to noise.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spyrit.core.prep import DirectPoisson\n",
    "from spyrit.core.recon import PinvNet\n",
    "from spyrit.core.noise import NoNoise\n",
    "from spyrit.core.train import train_model, Weight_Decay_Loss, load_net\n",
    "from spyrit.core.nnet import ConvNet\n",
    "from spyrit.core.meas import Linear\n",
    "\n",
    "\n",
    "test_amt = 4\n",
    "nbAngles = 20\n",
    "\n",
    "A = np.load(radon_matrix_path)\n",
    "\n",
    "Areduced = radonSpecifyAngles(A, nbAngles)\n",
    "meas = Linear(Areduced, True, reg = 1e-5)\n",
    "meas.h, meas.w = img_size, img_size\n",
    "\n",
    "noise = NoNoise(meas)        \n",
    "prep = DirectPoisson(1.0, meas)\n",
    "denoi = ConvNet()\n",
    "\n",
    "model = PinvNet(noise, prep, denoi)\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> **Q: How does `model` relate to the network architecture depicted above?**</font>\n",
    "\n",
    "<font color='green'>**Tip: Check the ouptput of `print(model)`**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**'acqu' represents the forward module that simulates measurements during the training phase; 'meas_op.H' represents the forward matrix $\\mathbf{A}$; 'pinv' compute the pseudo inverse solution; 'recon' represents the convolutional layers in the image domain**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.functional.to_grayscale,\n",
    "     transforms.Resize((img_size, img_size)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=data_root+\"train\", transform=transform)\n",
    "trainloader =     torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=False)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=data_root+\"test\", transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,shuffle=False)\n",
    "\n",
    "dataloaders = {'train': trainloader, 'val': testloader}\n",
    "inputs, labels = next(iter(dataloaders['val']))\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: What is the shape of the database 'input' variable? What does each dimension correspond to?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(XXX)  # COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**A: \\[Batch_size, channels, height, width\\]. We manipulate batches that contain 256 images of size 64 x 64; 'channels' = 1 indicates that images are grascale.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Explain the line 'transforms.Normalize([0.5], [0.5])'. Why is it important?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**A: We normalize the data to 0 mean and unit variance. Considering normalized data stabilizes and accelerates the optimization of the parameters of the network. Normalisation is also fondamental when the raw data extend over several orders of magnitude.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Training a reconstruction network from scratch (for 3 epochs)\n",
    "\n",
    "<font color='blue'>Run the cell below to train the convolutional layers. NB: training takes about two minutes</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "step_size=20\n",
    "gamma=0.2\n",
    "model_root='./data/nets/'\n",
    "\n",
    "loss = nn.MSELoss();\n",
    "criterion = Weight_Decay_Loss(loss);\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr);\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "model, train_info =  train_model(model, criterion, optimizer, scheduler, \n",
    "                                dataloaders, device, model_root, \n",
    "                                num_epochs=3, disp=True) \n",
    " \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Complete the code below to compare the ground-truth, pseudo-inverse, and the deep solutions.**</font>\n",
    "\n",
    "<font color='green'>**Tip1: Check the SPyrit documentation to compute the pseudoinverse solution from `model`.**</font>\n",
    "\n",
    "<font color='green'>**Tip2: Torch tensors may be in the GPU memory**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for index in range(test_amt):\n",
    "        \n",
    "        # Random image in STL10\n",
    "        np.random.seed(index)\n",
    "        i_test = np.random.randint(0, inputs.shape[0])\n",
    "       \n",
    "        \n",
    "        mes = model.acquire(inputs)\n",
    "        rec_pinv =  #Complete here: rec_pinv = model.XXXXX(mes).XXXX\n",
    "        rec_net =  #Complete here: rec_net = model.XXXXXXX(mes).XXXX\n",
    "        \n",
    "        # Plots\n",
    "        fig, axs = plt.subplots(1, 3, figsize =(20,10))\n",
    "        fig.suptitle('', fontsize=16)\n",
    "        \n",
    "        ax = axs[0]\n",
    "        ax.set_title(\"Ground-truth\")\n",
    "        aff = ax.imshow(inputs[i_test, 0, :, :].cpu(), cmap='gray')\n",
    "        fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "    \n",
    "        ax = axs[1]\n",
    "        ax.set_title(\"Pseudo-inverse \")\n",
    "        aff = ax.imshow(rec_pinv[i_test, 0, :, :], cmap='gray') \n",
    "        fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "    \n",
    "        ax = axs[2]\n",
    "        ax.set_title(\"Deep network\")\n",
    "        aff = ax.imshow(rec_net[i_test, 0, :, :], cmap='gray')\n",
    "        fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: How do the pseudo inverse solution and network output compare?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**A: After only a few epochs, the artefacts of the pseudo inverse solution are significantly reduced. However, the output of the networks is oversmooth**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Testing a trained model\n",
    "    \n",
    "Here we load networks that we have already trained for larger numbers of epochs\n",
    "    \n",
    "<font color='blue'> **Run the code below**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 100\n",
    "learning_rate = 1e-3\n",
    "step_size = 10\n",
    "gamma = 0.5\n",
    "batch_size = 512\n",
    "regularisation = 1e-7\n",
    "model_root='./data/nets/'\n",
    "\n",
    "nbAngles = 20\n",
    "\n",
    "inputs, labels = next(iter(dataloaders['val']))\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Network filename\n",
    "suffix = 'Q_{}_D_{}_T_{}_epo_{}_lr_{}_sss_{}_sdr_{}_bs_{}_reg_{}'.format(\n",
    "    img_size, pixel_size, nbAngles, num_epoch, \n",
    "    learning_rate, step_size, gamma, \n",
    "    batch_size, regularisation)\n",
    "title = model_root + 'NET_' + suffix\n",
    "\n",
    "# Loading the model\n",
    "Areduced = radonSpecifyAngles(A, nbAngles)\n",
    "meas = Linear(Areduced, True, reg = 1e-5)\n",
    "meas.h, meas.w = img_size, img_size\n",
    "noise = NoNoise(meas)        \n",
    "prep = DirectPoisson(1.0, meas)\n",
    "denoi = ConvNet()\n",
    "model = PinvNet(noise, prep, denoi)\n",
    "\n",
    "model = model.to(device)\n",
    "load_net(title, model, device = device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <font color='blue'> **Run the code below**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_amt = 4\n",
    "with torch.no_grad():\n",
    "    for index in range(test_amt):\n",
    "        # Choosing random image in STL10\n",
    "        np.random.seed(index)\n",
    "        i_test = np.random.randint(0, inputs.shape[0])\n",
    "        \n",
    "        \n",
    "        mes = model.acquire(inputs) \n",
    "        rec_pinv =  #Complete here: rec_pinv = model.XXXXX(mes)\n",
    "        rec_net =  #Complete here: rec_net = model.XXXXXX(mes).XXXXXX\n",
    "        \n",
    "        # Plots\n",
    "        fig, axs = plt.subplots(1, 3, figsize =(20,10))\n",
    "        fig.suptitle('', fontsize=16)\n",
    "        \n",
    "        ax = axs[0]\n",
    "        ax.set_title(\"Ground-truth\")\n",
    "        aff = ax.imshow(inputs[i_test, 0, :, :].cpu(), cmap='gray') \n",
    "        fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "        \n",
    "        ax = axs[1]\n",
    "        ax.set_title(\"Pseudo-inverse \")\n",
    "        aff = ax.imshow(rec_pinv[i_test, 0, :, :].cpu(), cmap='gray') \n",
    "        fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "        \n",
    "        ax = axs[2]\n",
    "        ax.set_title(\"Deep network\")\n",
    "        aff = ax.imshow(rec_net[i_test, 0, :, :].cpu(), cmap='gray') \n",
    "        fig.colorbar(aff, ax=ax,fraction=0.046,pad=0.04)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: What is the impact of training a deep neural network until convergence?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**A: The background exhibits much fewer artifacts while the object is sharper**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:brown\"> Influence of the number of projection angles\n",
    "    \n",
    "Here, we load several networks that we have already trained for different numbers of projection angles\n",
    "    \n",
    "<font color='blue'> **Complete and run the code below. You can also try to reconstruct different images in the batch**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_angles = [20, 40, 60]\n",
    "test_batch = 12\n",
    "c = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for nbAngles in list_angles:\n",
    "        \n",
    "        print(f\"Acquisition of {nbAngles} angles\")\n",
    "        \n",
    "        # Networks filename\n",
    "        suffix = 'Q_{}_D_{}_T_{}_epo_{}_lr_{}_sss_{}_sdr_{}_bs_{}_reg_{}'.format(                   \n",
    "            img_size, pixel_size, nbAngles, \n",
    "            num_epoch, learning_rate, step_size, gamma, batch_size, regularisation)\n",
    "        title = model_root + 'NET_' + suffix\n",
    "        \n",
    "        # loading model\n",
    "        Areduced = radonSpecifyAngles(A, nbAngles)\n",
    "        meas = Linear(Areduced, True, reg = 1e-5)\n",
    "        meas.h, meas.w = img_size, img_size\n",
    "        noise = NoNoise(meas)        \n",
    "        prep = DirectPoisson(1.0, meas)\n",
    "        denoi = ConvNet()\n",
    "        model = PinvNet(noise, prep, denoi)\n",
    "        \n",
    "        model = model.to(device)\n",
    "        load_net(title, model, device = device)\n",
    "        \n",
    "        #==========================================================================\n",
    "        mes = model.acquire(inputs)\n",
    "        rec_pinv = model.reconstruct_pinv(mes)\n",
    "        rec_net = model.reconstruct(mes).detach()\n",
    "\n",
    "        del model\n",
    "        \n",
    "        gt       = inputs[0, 0, :, :].cpu()\n",
    "        rec_pinv = rec_pinv[0, 0, :, :].cpu()\n",
    "        rec_net  = rec_net[0, 0, :, :].cpu()\n",
    "        \n",
    "        #=======================Plots==============================================\n",
    "        fig, axs = plt.subplots(1, 3, figsize =(20,10))\n",
    "        fig.suptitle('', fontsize=16)\n",
    "        \n",
    "        ax = axs[0]\n",
    "        ax.set_title(\"Ground-truth\")\n",
    "        aff = ax.imshow(gt, cmap='gray')\n",
    "\n",
    "        ax = axs[1]\n",
    "        ax.set_title(\"Pseudo-inverse\")\n",
    "        aff = ax.imshow(rec_pinv, cmap='gray')\n",
    "\n",
    "        ax = axs[2]\n",
    "        ax.set_title(\"Deep network\")\n",
    "        aff = ax.imshow(rec_net, cmap='gray') \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>**Q: Conclude on the limitations of deep-neural reconstructors.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**A: Deep-neural reconstructors cannot overcome the artefacts resulting from highly undersampled data (i.e., when no information is actually acquired)**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> **Conclusion**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this hands-on session, you should be able to\n",
    "* Understand the importance of modeling the forward operator of an inverse problem.\n",
    "* Reconstruct an image using a linear reconstructor (e.g., Moore-Penrose pseudo-inverse).\n",
    "* Understand the limitations of linear reconstruction methods for undersampled data (e.g., limited-angle tomography).\n",
    "* Reconstruct an image using a non-linear reconstructor (e.g., deep convolutional neural networks). \n",
    "* Integrate linear reconstructors into a deep-learning method.\n",
    "* Understand the impact of the training parameters (e.g., the choice of the number of epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "6bb91ed818fef5afd66e6530343cd83d0ba4f83bee28f2ba31796ff4f6ac86b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
